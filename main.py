#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Hotel Rating Aggregator
=======================

Main orchestrator that manages scrapers for multiple hotel sites:
- TripAdvisor (fully functional)
- Booking.com (fully functional)
- Google Travel (in development) 
- Decolar (fully functional)

Each site generates its own JSON file with structured data.
"""

import os
import json
import argparse
from datetime import datetime
from typing import Dict, List, Any, Optional

# Importar scrapers de cada site
from sites.tripadvisor.scraper import TripAdvisorScraper
from sites.booking.scraper import BookingScraper
from sites.google.scraper import GoogleTravelScraper
from sites.decolar.scraper import DecolarScraper


class HotelScrapingOrchestrator:
    """Orquestrador principal do sistema de scraping multi-sites"""
    
    def __init__(self, config_file: str = "config.env"):
        """
        Inicializa o orquestrador
        
        Args:
            config_file: Caminho para o arquivo de configura√ß√£o
        """
        self.config_file = config_file
        self.config = self._load_config()
        self.scrapers = self._initialize_scrapers()
        
    def _load_config(self) -> Dict[str, str]:
        """Carrega configura√ß√µes do arquivo .env"""
        config = {}
        
        if not os.path.exists(self.config_file):
            print(f"‚ùå Arquivo de configura√ß√£o n√£o encontrado: {self.config_file}")
            return config
            
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        config[key.strip()] = value.strip()
                        
            print(f"‚úÖ Configura√ß√µes carregadas: {len(config)} vari√°veis")
            return config
            
        except Exception as e:
            print(f"‚ùå Erro ao carregar configura√ß√µes: {e}")
            return {}
    
    def _initialize_scrapers(self) -> Dict[str, Any]:
        """Inicializa scrapers para todos os sites"""
        return {
            'tripadvisor': TripAdvisorScraper(),
            'booking': BookingScraper(),
            'google': GoogleTravelScraper(),
            'decolar': DecolarScraper()
        }
    
    def _get_hotels_config_for_site(self, site: str) -> Dict[str, str]:
        """
        Extrai configura√ß√£o de hot√©is para um site espec√≠fico
        
        Args:
            site: Nome do site (tripadvisor, booking, google, decolar)
            
        Returns:
            Dict com {nome_hotel: url_hotel}
        """
        hotels_config = {}
        site_upper = site.upper()
        
        # Busca dinamicamente todos os hot√©is configurados para o site
        # Padr√£o: SITE_HOTEL_NAME=url
        for key, value in self.config.items():
            if key.startswith(f"{site_upper}_") and not key.endswith("_ID"):
                # Extrai o nome do hotel da chave de configura√ß√£o
                hotel_key = key.replace(f"{site_upper}_", "")
                
                # Converte o nome da chave para formato de exibi√ß√£o
                hotel_name = self._format_hotel_name(hotel_key)
                
                hotels_config[hotel_name] = value
        
        return hotels_config
    
    def _format_hotel_name(self, hotel_key: str) -> str:
        """
        Converte chave de configura√ß√£o em nome de exibi√ß√£o do hotel
        
        Args:
            hotel_key: Chave do hotel (ex: MARAGOGI_BRISA_EXCLUSIVE)
            
        Returns:
            Nome formatado do hotel (ex: Maragogi Brisa Exclusive)
        """
        # Converte de UPPER_CASE para Title Case
        words = hotel_key.lower().replace('_', ' ').split()
        formatted_words = []
        
        for word in words:
            # Palavras especiais que ficam em min√∫sculo
            if word in ['de', 'da', 'do', 'das', 'dos', 'e']:
                formatted_words.append(word)
            else:
                formatted_words.append(word.capitalize())
        
        # Se n√£o come√ßar com "Hotel", adiciona se apropriado
        formatted_name = ' '.join(formatted_words)
        if not formatted_name.lower().startswith('hotel') and not any(formatted_name.lower().startswith(prefix) for prefix in ['maragogi', 'pousada', 'resort']):
            formatted_name = f"Hotel {formatted_name}"
        
        return formatted_name
    
    def _save_results_to_json(self, site: str, results: List[Dict[str, Any]]) -> str:
        """
        Salva resultados em arquivo JSON espec√≠fico do site
        
        Args:
            site: Nome do site
            results: Lista com dados dos hot√©is
            
        Returns:
            Caminho do arquivo salvo
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"resultados/{site}_dados_{timestamp}.json"
        
        # Garante que a pasta existe
        os.makedirs("resultados", exist_ok=True)
        
        # Estrutura final do JSON
        output_data = {
            "metadata": {
                "site": site,
                "total_hoteis": len(results),
                "timestamp_extracao": datetime.now().isoformat(),
                "versao_scraper": "2.0.0-multi-site"
            },
            "hoteis": results
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            print(f"üíæ Dados salvos: {filename}")
            return filename
            
        except Exception as e:
            print(f"‚ùå Erro ao salvar arquivo: {e}")
            return ""
    
    def scrape_site(self, site: str) -> Optional[str]:
        """
        Executa scraping para um site espec√≠fico
        
        Args:
            site: Nome do site a ser processado
            
        Returns:
            Caminho do arquivo JSON gerado ou None se erro
        """
        if site not in self.scrapers:
            print(f"‚ùå Site n√£o suportado: {site}")
            return None
        
        print(f"\nüöÄ Iniciando scraping do {site.upper()}")
        print("=" * 50)
        
        # Obt√©m configura√ß√£o de hot√©is para o site
        hotels_config = self._get_hotels_config_for_site(site)
        
        if not hotels_config:
            print(f"‚ùå Nenhum hotel configurado para {site}")
            return None
        
        print(f"üè® Hot√©is configurados para {site}: {len(hotels_config)}")
        
        # Executa scraping
        scraper = self.scrapers[site]
        results = scraper.scrape_multiple_hotels(hotels_config)
        
        if results:
            # Salva resultados
            json_file = self._save_results_to_json(site, results)
            print(f"‚úÖ {site.upper()} conclu√≠do: {len(results)} hot√©is processados")
            return json_file
        else:
            print(f"‚ùå Nenhum resultado obtido para {site}")
            return None
    
    def scrape_all_sites(self) -> Dict[str, str]:
        """
        Executa scraping para todos os sites configurados
        
        Returns:
            Dict com {site: caminho_arquivo_json}
        """
        results = {}
        available_sites = ['tripadvisor', 'booking', 'google', 'decolar']
        
        print("üéØ INICIANDO SCRAPING MULTI-SITE")
        print("=" * 60)
        print(f"Sites dispon√≠veis: {', '.join(available_sites)}")
        print(f"Total de hot√©is por site: {len(self._get_hotels_config_for_site('tripadvisor'))}")
        
        for site in available_sites:
            try:
                json_file = self.scrape_site(site)
                if json_file:
                    results[site] = json_file
                    
                # Pequena pausa entre sites
                if site != available_sites[-1]:  # N√£o espera no √∫ltimo
                    print(f"‚è≥ Pausa entre sites...")
                    import time
                    time.sleep(2)
                    
            except Exception as e:
                print(f"‚ùå Erro ao processar {site}: {e}")
                continue
        
        return results
    
    def show_status(self):
        """Mostra status das configura√ß√µes e scrapers"""
        print("üìä STATUS DO SISTEMA")
        print("=" * 40)
        
        # Status das configura√ß√µes
        print(f"üìÅ Arquivo de config: {self.config_file}")
        print(f"‚öôÔ∏è  Configura√ß√µes carregadas: {len(self.config)}")
        
        # Status dos scrapers
        print(f"\nü§ñ Scrapers dispon√≠veis:")
        for site, scraper in self.scrapers.items():
            implemented = getattr(scraper, 'implemented', True)
            status = "‚úÖ Funcional" if implemented else "üöß Em desenvolvimento"
            print(f"   {site.capitalize()}: {status}")
        
        # Status dos hot√©is configurados
        print(f"\nüè® Hot√©is configurados por site:")
        for site in self.scrapers.keys():
            hotels = self._get_hotels_config_for_site(site)
            print(f"   {site.capitalize()}: {len(hotels)} hot√©is")


def main():
    """Fun√ß√£o principal com argumentos de linha de comando"""
    parser = argparse.ArgumentParser(
        description="Sistema Multi-Site de Scraping de Hot√©is",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  python main.py                    # Executa todos os sites
  python main.py --site tripadvisor # Executa apenas TripAdvisor
  python main.py --status           # Mostra status do sistema
  python main.py --sites booking decolar # Executa sites espec√≠ficos
        """
    )
    
    parser.add_argument(
        '--site', 
        choices=['tripadvisor', 'booking', 'google', 'decolar'],
        help='Executa scraping para um site espec√≠fico'
    )
    
    parser.add_argument(
        '--sites',
        nargs='+',
        choices=['tripadvisor', 'booking', 'google', 'decolar'],
        help='Executa scraping para m√∫ltiplos sites espec√≠ficos'
    )
    
    parser.add_argument(
        '--status',
        action='store_true',
        help='Mostra status das configura√ß√µes e scrapers'
    )
    
    parser.add_argument(
        '--config',
        default='config.env',
        help='Caminho para arquivo de configura√ß√£o (padr√£o: config.env)'
    )
    
    args = parser.parse_args()
    
    # Inicializa orquestrador
    orchestrator = HotelScrapingOrchestrator(args.config)
    
    # Executa a√ß√£o solicitada
    if args.status:
        orchestrator.show_status()
        
    elif args.site:
        # Executa site espec√≠fico
        json_file = orchestrator.scrape_site(args.site)
        if json_file:
            print(f"\nüéØ Arquivo gerado: {json_file}")
            
    elif args.sites:
        # Executa m√∫ltiplos sites espec√≠ficos
        results = {}
        for site in args.sites:
            json_file = orchestrator.scrape_site(site)
            if json_file:
                results[site] = json_file
        
        print(f"\nüéØ Arquivos gerados: {len(results)}")
        for site, file_path in results.items():
            print(f"   {site.capitalize()}: {file_path}")
            
    else:
        # Executa todos os sites (padr√£o)
        results = orchestrator.scrape_all_sites()
        
        print(f"\nüéØ SCRAPING CONCLU√çDO")
        print("=" * 30)
        print(f"Sites processados: {len(results)}")
        
        for site, file_path in results.items():
            print(f"‚úÖ {site.capitalize()}: {file_path}")


if __name__ == "__main__":
    main() 